{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 - Analyzing Food Reviews\n",
    "\n",
    "### Introduction to Natural Language Processing (NLP)\n",
    "\n",
    "- Automatic computational processing of human languages\n",
    "\n",
    "- Functions:\n",
    "    - Take and understand text data\n",
    "    - Generate natural looking text\n",
    "\n",
    "- Language is unstructured and raw\n",
    "</br></br>\n",
    "\n",
    "<img src=\"./images/news.png\" width=\"700\">\n",
    "\n",
    "##### **Why is NLP hard?**\n",
    "- Language is highly variable and ambiguous\n",
    "- Language is symbolic, discrete and sparse.\n",
    "- However, humans cannot define rules that govern language\n",
    "\n",
    "<img src=\"./images/pizza.png\" width=\"800\">\n",
    "\n",
    "\n",
    "#### Online Video Recap:\n",
    "\n",
    "- NLP and Applications\n",
    "- NLP Corpora and Packages\n",
    "- Tokenization\n",
    "- Removing Stopwords\n",
    "- Stemming and Lemmatization\n",
    "- Synonyms, Antonyms, Hypernyms, Hyponyms\n",
    "- Exploratory Data Analysis for Text:\n",
    "    - Word Cloud\n",
    "    - Word Vectors\n",
    "    - Dimensionality Reduction and Visualization\n",
    "\n",
    "#### <span style='color:blue'>Problem: Analyze Food Reviews</span> \n",
    "\n",
    "Tasks for this exercise:\n",
    "1. Clean Food Reviews Data\n",
    "2. Analyze most frequent words\n",
    "3. Anaylyze words by rating\n",
    "4. Use synonyms for seach and analysis\n",
    "5. Create new features out from raw text\n",
    "6. Create simple Bag of Words Features\n",
    "7. Create Word Vectors\n",
    "8. Visualize Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='color:blue'>Task 1: Fetch Food Reviews Data</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "import pandas as pd\n",
    "# read only first 5 columns\n",
    "food_reviews = pd.read_csv(\"reviews.csv\", usecols=[\"Restaurant\", \"Reviewer\", \"Review\", \"Rating\", \"Time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info and summary\n",
    "print(food_reviews.shape)\n",
    "food_reviews.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "Tokenization is a fundamental step in natural language processing (NLP) that involves breaking down a stream of text into smaller units called tokens, which can be words, subwords, or characters.\n",
    "\n",
    "- Tokenization assisst in text normalization and helps in converting text into a structured format that is easier to analyze.\n",
    "- Simplifies complex text as tokenized text is more manageable and less ambiguous\n",
    "- Tokenization allows for the extraction of features from text data, such as word frequency, n-grams, and term-document matrices.\n",
    "- Tokenized text is easier to analyze statistically, allowing for the calculation of metrics such as word frequency, co-occurrence, and sentiment analysis.\n",
    "\n",
    "#### <span style='color:blue'>Task 2: Tokenize the Reviews</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize reviews\n",
    "tokenized_reviews = food_reviews.loc[:, \"Review\"].str.split()\n",
    "tokenized_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punkt is a data-driven, unsupervised machine learning system for tokenizing text into sentences and words. It is particularly robust and can handle a variety of languages and text formats. The Punkt models are **pre-trained on a large corpus of text, allowing them to accurately identify sentence boundaries and tokenize text effectively**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "# download the Punkt tokenizer models from NLTK library\n",
    "nltk.download('punkt')\n",
    "\n",
    "tokenized_reviews = food_reviews.loc[:, \"Review\"].astype(str).apply(word_tokenize).copy()\n",
    "tokenized_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the string `split()` function and using NLTK's `word_tokenize` function both serve the purpose of tokenizing text, but they do so in different ways and offer different levels of sophistication and accuracy. Here are the key differences and benefits:\n",
    "\n",
    "##### String `split()` Function\n",
    "\n",
    "- The `split()` function splits a string based on a specified delimiter (default is any whitespace).\n",
    "- It does not take into account linguistic rules or the context of the text.\n",
    "- Does not handle punctuation correctly (e.g., \"world!\" remains \"world!\").\n",
    "- Does not recognize contractions or other complex linguistic structures.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello, world! How's it going?\"\n",
    "tokens = text.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### NLTK `word_tokenize` Function\n",
    "\n",
    "- The `word_tokenize` function uses a more sophisticated approach to tokenization, leveraging linguistic rules and models.\n",
    "- It handles punctuation, contractions, and other complexities in the text more accurately (e.g., splits \"world!\" into \"world\" and \"!\").\n",
    "- Recognizes contractions and splits them appropriately (e.g., \"How's\" into \"How\" and \"'s\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"Hello, world! How's it going?\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Stop Words\n",
    "\n",
    "Stop words are common words that typically do not carry significant meaning and are often filtered out to focus on the more informative parts of the text.\n",
    "\n",
    "How does removing stop words help?\n",
    "- Removing stop words decreases the number of unique words in the text, reducing the dimensionality of the text data.\n",
    "- Stop words often add noise to the text data without contributing meaningful information. Removing them improves the signal-to-noise ratio\n",
    "- Eliminates words that do not contribute to the predictive power of the model.\n",
    "- The focus shifts to more meaningful and content-rich words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='color:blue'>Task 3: Remove stopwords and punctuation</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your custom stop words here\n",
    "custom_stop_words = {'food', 'dinner'}  \n",
    "all_stop_words = stop_words.union(custom_stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower case strings\n",
    "tokenized_reviews = tokenized_reviews.apply(lambda x: [word.lower() for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove stop words\n",
    "def remove_stop_words(tokens):\n",
    "    return [word for word in tokens if word.lower() not in all_stop_words]\n",
    "\n",
    "food_reviews_no_stop = tokenized_reviews.apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# food_reviews.head()\n",
    "food_reviews_no_stop.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punctuation marks do not carry semantic meaning, and removing them simplifies the text, making it easier to process and analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuations\n",
    "import string\n",
    "def remove_punctuation(tokens):\n",
    "    return [word for word in tokens if word not in string.punctuation]\n",
    "\n",
    "# Apply the function to the tokenized series\n",
    "food_reviews_no_punct = food_reviews_no_stop.apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_reviews_no_punct.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Stemming and Lemmatization\n",
    "\n",
    "**Stemming** is the process of reducing a word to its base or root form. The resulting stem may not be a valid word, and stemming algorithms typically use heuristic rules to strip suffixes from words.\n",
    "\n",
    "**Lemmatization** is the process of reducing a word to its base or dictionary form, known as the lemma. Lemmatization considers the context and the part of speech of the word to determine its lemma. It uses linguistic rules and dictionaries.\n",
    "\n",
    "Lemmatization is more accurate and preferred for tasks requiring accurate word forms and deeper linguistic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='color:blue'>Task 4: Lemmatize the tokens</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for word in ['the', 'children', 'are', 'eating']:\n",
    "    print(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to lemmatize tokens\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Apply the function to the tokenized series\n",
    "food_reviews_lemmatized = food_reviews_no_punct.apply(lemmatize_tokens)\n",
    "food_reviews_lemmatized.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "### Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a word cloud\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, contour_color='steelblue')\n",
    "wordcloud.generate(str(food_reviews_lemmatized))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='color:blue'>Task 5: Perform basic EDA on Reviews data</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word frequcency count of top 10 words\n",
    "from collections import Counter\n",
    "words = []\n",
    "for i in food_reviews_lemmatized:\n",
    "    words.extend(i)\n",
    "word_count = Counter(words)\n",
    "word_count.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot top 10 words\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(*zip(*word_count.most_common(10)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the distribution of review length\n",
    "food_reviews.loc[:, 'Review'].str.len().hist(bins=100, range=(0, 1500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='color:blue'>Task 6: WordCloud for the best and worse reviews</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numbers\n",
    "food_reviews.loc[:, \"Rating\"] = pd.to_numeric(food_reviews.loc[:, \"Rating\"], errors='coerce')\n",
    "food_reviews.loc[:, \"Rating\"] = food_reviews.loc[:, \"Rating\"].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_review_index = food_reviews.loc[food_reviews[\"Rating\"] > 4].index\n",
    "low_review_index = food_reviews.loc[food_reviews[\"Rating\"] < 2].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, contour_color='steelblue')\n",
    "wordcloud.generate(str(food_reviews_lemmatized[high_review_index]))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, contour_color='steelblue')\n",
    "wordcloud.generate(str(food_reviews_lemmatized[low_review_index]))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synonym Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Example word\n",
    "word = \"delicious\"\n",
    "\n",
    "# Get synonyms\n",
    "synonyms = wordnet.synsets(word)\n",
    "synonym_list = set()\n",
    "for syn in synonyms:\n",
    "    for lemma in syn.lemmas():\n",
    "        synonym_list.add(lemma.name())\n",
    "\n",
    "print(synonym_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_reviews(query, reviews):\n",
    "    synonyms = wordnet.synsets(query)\n",
    "    synonym_list = set()\n",
    "    for syn in synonyms:\n",
    "        for lemma in syn.lemmas():\n",
    "            synonym_list.add(lemma.name())\n",
    "\n",
    "    results = []\n",
    "    result_idx = []\n",
    "    for idx, review in enumerate(reviews):\n",
    "        if any(synonym in review for synonym in synonym_list):\n",
    "            results.append(review)\n",
    "            result_idx.append(idx)\n",
    "    return results, result_idx\n",
    "\n",
    "query = \"delicious\"\n",
    "results, result_idx = search_reviews(query, food_reviews_lemmatized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_reviews.loc[result_idx[1], 'Review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction and Structuring in Text Analysis\n",
    "\n",
    "Link to the case study: [Airbnb Example](https://medium.com/airbnb-engineering/prioritizing-home-attributes-based-on-guest-interest-3c49b827e51a)\n",
    "\n",
    "<img src=\"./images/airbnb.png\" width=\"600\">\n",
    "\n",
    "#### <span style='color:blue'>Task 7: Extract Boolean attributes from raw reviews</span> \n",
    "The task is to pick attributes about the restaurant in 5 main cateogies and check if the reviews mention any of the keywords within those cateogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_reviews.loc[:, 'Review_Processed'] = food_reviews_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_keywords = {\n",
    "    'good_environment': ['ambience', 'environment', 'atmosphere', 'decor'],\n",
    "    'good_service': ['service', 'staff', 'waiter', 'waitress', 'server'],\n",
    "    'good_food': ['delicious', 'tasty', 'yummy', 'flavorful'],\n",
    "    'expensive': ['expensive', 'pricey', 'costly'],\n",
    "    'good_value': ['cheap', 'inexpensive', 'affordable', 'value']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tokens, feature_keywords):\n",
    "    features = {}\n",
    "    for feature, keywords in feature_keywords.items():\n",
    "        features[feature] = any(token.lower() in keywords for token in tokens)\n",
    "    return features\n",
    "\n",
    "# Apply feature extraction to each review\n",
    "feature_df = food_reviews.loc[:, 'Review_Processed'].apply(lambda tokens: extract_features(tokens, feature_keywords))\n",
    "features_df = pd.DataFrame.from_records(feature_df.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_reviews = pd.concat([food_reviews, features_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_reviews.iloc[:10, [2,6,7,8,9,10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Generate feature keywords dictionary using synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Text Representation\n",
    "\n",
    "Representing text in a numerical format is a fundamental step in natural language processing (NLP). Here are some of the most common methods for representing text, including both traditional and modern techniques:\n",
    "\n",
    "#### 1. Bag of Words (BoW)\n",
    "\n",
    "- Represents text by the presence or absence (or frequency) of words.\n",
    "- Constructs a vocabulary from all the unique words in the corpus.\n",
    "\n",
    "**Uses:**</br>\n",
    "- Sentiment Analysis: Determine the sentiment (positive or negative) of a document by counting the occurrences of positive and negative words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample text\n",
    "corpus = [\"This is the first document.\", \"This document is the second document.\", \"And this is the third one.\"]\n",
    "\n",
    "# Create the BoW model\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Print the vocabulary and the document-term matrix\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='color:blue'>Task 8: Create a Bag of Words from Food Reviews</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels based on ratings\n",
    "food_reviews.loc[:, 'label'] = food_reviews.loc[:, 'Rating'].apply(lambda x: 1 if x > 3 else 0)\n",
    "y = food_reviews.loc[:, 'label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_reviews = [' '.join(review) for review in food_reviews_lemmatized]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the text data\n",
    "X = vectorizer.fit_transform(joined_reviews)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='color:blue'>Task 9: Train and test sentiment analysis model with BoW</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Initialize and train the classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict a single review\n",
    "review = food_reviews_lemmatized[10]\n",
    "review_vector = vectorizer.transform([' '.join(review)])\n",
    "print(food_reviews.loc[10, 'Review'])\n",
    "print(clf.predict(review_vector))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_reviews['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 2. Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "**Description:**\n",
    "- Adjusts the frequency of words by how common or rare they are across all documents.\n",
    "- TF measures how frequently a word appears in a document.\n",
    "- IDF measures how important a word is in the entire corpus.\n",
    "\n",
    "\n",
    "**Uses:**\n",
    "- Document Similarity: Measure similarity between documents, useful in clustering and recommendation systems.\n",
    "- Keyword Extraction: Identify important words in a document by giving less importance to common words that appear in many documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 3. Word Embeddings (Word Vectors)\n",
    "\n",
    "**Description:**\n",
    "- Dense vector representations of words.\n",
    "- Capture semantic meaning by placing similar words close to each other in the vector space.\n",
    "- Common models include Word2Vec, GloVe, and FastText.\n",
    "\n",
    "\n",
    "<img src=\"./images/embeddings.png\" width=\"800\">\n",
    "</br>Image Source: nlplanet.org/course-practical-nlp\n",
    "\n",
    "\n",
    "**Uses:**\n",
    "- Semantic Similarity: Determine how similar two words or phrases are in meaning.\n",
    "- Machine Translation: Translate words and phrases by finding equivalent representations in different languages.\n",
    "- Sentiment Analysis: Capture context and nuances in text by understanding word meanings and relationships.\n",
    "\n",
    "\n",
    "#### <span style='color:blue'>Task 10: Create Embeddings</span> \n",
    "\n",
    "**Word2Vec**, developed by the Google research team led by Tomas Mikolov, is a popular technique for learning word embeddings from text. It's an unsupervised learning model that uses a neural network to produce dense, continuous vector representations of words, capturing their semantic meanings and relationships.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [[\"this\", \"is\", \"the\", \"first\", \"document\"],\n",
    "             [\"this\", \"is\", \"the\", \"second\", \"document\"],\n",
    "             [\"and\", \"this\", \"is\", \"the\", \"third\", \"one\"]]\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=50, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Print the vector for the word 'document'\n",
    "print(model.wv['document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load GloVe vectors\n",
    "glove_vectors = {}\n",
    "with open('glove/glove.6B.50d.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        glove_vectors[word] = vector\n",
    "\n",
    "# Example: Get the vector for the word 'food' from GloVe\n",
    "glove_food_vector = glove_vectors.get('food')\n",
    "print(glove_food_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cosine similarity\n",
    "from scipy.spatial import distance\n",
    "\n",
    "cosine_similarity = 1 - distance.cosine(glove_vectors.get('dinner'), glove_vectors.get('lunch'))\n",
    "print(f'Cosine similarity: {cosine_similarity}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### 4. One-Hot Encoding\n",
    "\n",
    "**Description:**\n",
    "- Represents words as vectors with the same length as the vocabulary.\n",
    "- Each word is represented by a vector with a 1 in the position corresponding to the wordâ€™s index in the vocabulary and 0s elsewhere.\n",
    "\n",
    "**Uses:**\n",
    "- Text Classification: Simple representation for short and well-defined text categories, such as document classification.\n",
    "- Sequence Models: Input for neural network models in sequence-to-sequence tasks like language modeling.\n",
    "\n",
    "<img src=\"./images/onehot.png\" width=\"500\">\n",
    "</br>Image Source: medium.com/analytics-vidhya/one-hot-encoding-of-text-data-in-natural-language-processing-2242fefb2148\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### 5. n-Grams\n",
    "\n",
    "**Description:**\n",
    "- Represents text by considering sequences of n words together.\n",
    "- Can be used to capture context and local word order.\n",
    "\n",
    "\n",
    "**Uses:**\n",
    "- Text Generation: Generate text by predicting the next word based on the previous n words.\n",
    "- Spell Correction: Identify common misspellings by analyzing sequences of characters.\n",
    "\n",
    "\n",
    "#### Summary\n",
    "\n",
    "- **Bag of Words (BoW):** Simple and effective for basic tasks.\n",
    "- **TF-IDF:** Adds importance weighting to words.\n",
    "- **Word Embeddings (Word2Vec, GloVe, FastText):** Capture semantic similarity and context.\n",
    "- **One-Hot Encoding:** Simple but high-dimensional.\n",
    "- **n-Grams:** Capture local word order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Applications of these techniques\n",
    "\n",
    "- Sentiment Analysis\n",
    "- Topic Modeling\n",
    "- Named Entity Recognition\n",
    "- Text Classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
